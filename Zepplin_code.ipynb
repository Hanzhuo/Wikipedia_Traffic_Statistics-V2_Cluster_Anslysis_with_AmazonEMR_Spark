{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Python scripts to get the number of links directed from other web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%python\n",
    "index_name = dict()\n",
    "f1=open('/data/wikilinks/titles-sorted.txt','r')\n",
    "index = 0\n",
    "for name in f1:\n",
    "    index += 1\n",
    "    name = name.strip()\n",
    "    index_name[index] = name\n",
    "\n",
    "f1.close()\n",
    "\n",
    "#create dictionary to store page index as key and the number of other pages input\n",
    "index_inputWebNum = dict()\n",
    "f2=open('/data/wikilinks/links-simple-sorted.txt')\n",
    "for line in f2:\n",
    "    line = line.strip()\n",
    "    from_to = line.split(': ')\n",
    "    indexs = from_to[1].split(' ')\n",
    "    for index in indexs:\n",
    "        index = int(index)\n",
    "        if index in index_inputWebNum:\n",
    "            index_inputWebNum[index] += 1\n",
    "        else:\n",
    "            index_inputWebNum[index] = 1\n",
    "\n",
    "f2.close()\n",
    "\n",
    "#write page name and the number of other pages input For each line\n",
    "f = open('/data/wikilinks/name_inputNum.txt','w')\n",
    "for i in range(len(index_name)):\n",
    "    index = i + 1\n",
    "    if index in index_inputWebNum:\n",
    "        f.writelines(str(index_name[index]) + ' ' + str(index_inputWebNum[index]) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "#read one week uncompressed files\n",
    "files=sc.textFile(\"s3://678wikiclusterdata/678wikiclusterdata/\")\n",
    "#Process data and convert to dataframe\n",
    "x =  files.map(lambda line: line.split(\" \"))\n",
    "df = x.toDF(['project', 'name', 'views', 'bytes'])\n",
    "\n",
    "#convert project names (string) to numbers (int)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"project\", outputCol=\"project_index\", handleInvalid='error')\n",
    "model = stringIndexer.fit(df)\n",
    "td = model.transform(df)\n",
    "td.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "td.registerTempTable(\"dfData\")\n",
    "#use sql group by project_index and name to calculate sum of views and mean of bytes for each page \n",
    "viewset = spark.sql(\"SELECT project_index, name, sum(views) AS views, avg(bytes) AS bytes FROM dfData GROUP BY project_index, name\")\n",
    "#check the statistical description\n",
    "#viewset.describe('views','bytes').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "viewset.registerTempTable(\"dfview\")\n",
    "#get the mean value of views\n",
    "highview = spark.sql(\"SELECT * FROM dfview WHERE views > 19.75\")\n",
    "#highview.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "name_inputNum=sc.textFile(\"s3://678wikiclusterdata/name_inputNum.txt\")\n",
    "x =  name_inputNum.map(lambda line: line.split(\" \"))\n",
    "name_input_df = x.toDF(['name', 'inputNum'])\n",
    "\n",
    "#combine highview and name_input_df dataframes with sql join function\n",
    "finalDF = highview.join(name_input_df, 'name', 'left').select(highview.name, highview.views, highview.bytes, name_input_df.inputNum, highview.project_index)\n",
    "#replace null value with 0\n",
    "finalDF = finalDF.na.fill({'inputNum': 0})\n",
    "finalDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "finalDF.describe('views','bytes', 'inputNum', 'project_index').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "finalDF1 = finalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import max\n",
    "views_min = finalDF1.select(min('views').alias('min_views')).collect()[0]['min_views']\n",
    "views_max = finalDF1.select(max('views').alias('max_views')).collect()[0]['max_views']\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "Min_v = views_min\n",
    "Max_v = views_max\n",
    "#user define function\n",
    "Norm_views_function = udf(lambda v: (v - Min_v) / (Max_v - Min_v), DoubleType())\n",
    "finalDF2 = finalDF1.withColumn('Norm_views', Norm_views_function(finalDF1.views))\n",
    "#Norm_views = finalDF1.select(Norm_views_function(finalDF1.views).alias('Norm_views'))\n",
    "\n",
    "bytes_min = finalDF1.select(min('bytes').alias('min_bytes')).collect()[0]['min_bytes']\n",
    "bytes_max = finalDF1.select(max('bytes').alias('max_bytes')).collect()[0]['max_bytes']\n",
    "Min_v = bytes_min\n",
    "Max_v = bytes_max\n",
    "Norm_bytes_function = udf(lambda v: (v - Min_v) / (Max_v - Min_v), DoubleType())\n",
    "finalDF2 = finalDF2.withColumn('Norm_bytes', Norm_bytes_function(finalDF2.bytes))\n",
    "\n",
    "inputNum_min = float(finalDF1.select(min('inputNum').alias('min_inputNum')).collect()[0]['min_inputNum'])\n",
    "inputNum_max = float(finalDF1.select(max('inputNum').alias('max_inputNum')).collect()[0]['max_inputNum'])\n",
    "Min_v = inputNum_min\n",
    "Max_v = inputNum_max\n",
    "Norm_inputNum_function = udf(lambda v: (float(v) - Min_v) / (Max_v - Min_v), DoubleType())\n",
    "finalDF2 = finalDF2.withColumn('Norm_inputNum', Norm_inputNum_function(finalDF2.inputNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "#conduct OneHotEncoder for project_index column\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "finalDF2.registerTempTable(\"dfData\")\n",
    "finalDF2 = spark.sql(\"SELECT name, Norm_views, Norm_bytes, Norm_inputNum, project_index FROM dfData\")\n",
    "\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"project_index\", outputCol=\"project_Vec\")\n",
    "encoded = encoder.transform(finalDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "encoded.registerTempTable(\"dfData\")\n",
    "finalDF3 = spark.sql(\"SELECT Norm_views, Norm_bytes, Norm_inputNum, project_Vec FROM dfData\")\n",
    "\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "import numpy as np\n",
    "#824 should be revised according to your onehotcode result\n",
    "RDD = finalDF3.rdd.map(lambda line: SparseVector(824, line[\"project_Vec\"].indices.tolist() + [821, 822, 823], line[\"project_Vec\"].values.tolist() + [line[\"Norm_views\"], line[\"Norm_bytes\"], line[\"Norm_inputNum\"]])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "clusters = KMeans.train(RDD, 2, maxIterations=10, runs=10, initializationMode=\"k-means||\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
